import json
from typing import Dict, List, Tuple

import numpy as np
from dotenv import load_dotenv
from src.Agent import Agent
from src.Embedding import Embedding

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# === Step 1: åˆå§‹åŒ– ===
print("=== å®Œæ•´RAGç³»çµ±å¯¦ä½œ ===")
print()

# === Step 2: æ–‡ä»¶è™•ç†é¡ ===


class DocumentProcessor:
    """æ–‡ä»¶è™•ç†å™¨"""

    @staticmethod
    def split_text(text: str, chunk_size: int = 200) -> List[str]:
        """
        å°‡é•·æ–‡æœ¬åˆ‡åˆ†æˆå°å¡Š
        """
        words = text.split()
        chunks = []
        current_chunk = []
        current_size = 0

        for word in words:
            current_chunk.append(word)
            current_size += len(word) + 1

            if current_size >= chunk_size:
                chunks.append(" ".join(current_chunk))
                current_chunk = []
                current_size = 0

        if current_chunk:
            chunks.append(" ".join(current_chunk))

        return chunks


# === Step 3: å‘é‡è³‡æ–™åº« ===
class VectorDatabase:
    """ç°¡å–®çš„å‘é‡è³‡æ–™åº«"""

    def __init__(self):
        self.documents = []
        self.embeddings = []
        self.metadata = []

    def add_document(self, text: str, embedding: List[float], metadata: Dict = None):
        """æ·»åŠ æ–‡ä»¶åˆ°è³‡æ–™åº«"""
        self.documents.append(text)
        self.embeddings.append(embedding)
        self.metadata.append(metadata or {})

    def search(
        self, query_embedding: List[float], top_k: int = 3
    ) -> List[Tuple[str, float, Dict]]:
        """å‘é‡æœå°‹"""
        similarities = []

        for i, doc_embedding in enumerate(self.embeddings):
            sim = self._cosine_similarity(query_embedding, doc_embedding)
            similarities.append((self.documents[i], sim, self.metadata[i]))

        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

    def save(self, filename: str):
        """å„²å­˜è³‡æ–™åº«"""
        data = {
            "documents": self.documents,
            "embeddings": self.embeddings,
            "metadata": self.metadata,
        }
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"è³‡æ–™åº«å·²å„²å­˜åˆ° {filename}")

    def load(self, filename: str):
        """è¼‰å…¥è³‡æ–™åº«"""
        with open(filename, "r", encoding="utf-8") as f:
            data = json.load(f)
        self.documents = data["documents"]
        self.embeddings = data["embeddings"]
        self.metadata = data["metadata"]
        print(f"å¾ {filename} è¼‰å…¥äº† {len(self.documents)} å€‹æ–‡ä»¶")


# === Step 4: RAGç³»çµ±ä¸»é¡ ===
class RAGSystem:
    """å®Œæ•´çš„RAGç³»çµ±"""

    def __init__(self, agent, embedder):
        self.vector_db = VectorDatabase()
        self.agent = agent
        self.embedder = embedder
        self.doc_processor = DocumentProcessor()

    def add_knowledge(self, text: str, source: str = "unknown"):
        """æ·»åŠ çŸ¥è­˜åˆ°ç³»çµ±"""
        print(f"æ·»åŠ çŸ¥è­˜ä¾†æºï¼š{source}")

        # åˆ‡åˆ†æ–‡æœ¬
        chunks = self.doc_processor.split_text(text)
        print(f"  åˆ‡åˆ†æˆ {len(chunks)} å€‹ç‰‡æ®µ")

        # ç‚ºæ¯å€‹ç‰‡æ®µç”Ÿæˆå‘é‡ä¸¦å„²å­˜
        for i, chunk in enumerate(chunks):
            # ç”Ÿæˆå‘é‡
            embedding = self.embedder(chunk)

            # æ·»åŠ åˆ°è³‡æ–™åº«
            self.vector_db.add_document(
                text=chunk,
                embedding=embedding,
                metadata={"source": source, "chunk_id": i},
            )

    def answer_question(self, question: str) -> Dict:
        """å›ç­”å•é¡Œ"""
        print(f"\nå•é¡Œï¼š{question}")

        # Step 1: ç”Ÿæˆå•é¡Œå‘é‡
        query_embedding = self.embedder(question)

        # Step 2: æª¢ç´¢ç›¸é—œæ–‡ä»¶
        results = self.vector_db.search(query_embedding, top_k=3)

        if not results:
            return {
                "answer": "æŠ±æ­‰ï¼Œæˆ‘æ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ä¾†å›ç­”ä½ çš„å•é¡Œã€‚",
                "sources": [],
                "confidence": 0.0,
            }

        # Step 3: æº–å‚™ä¸Šä¸‹æ–‡
        context_parts = []
        sources = []
        total_score = 0

        for doc, score, meta in results:
            context_parts.append(f"è³‡æ–™{len(context_parts)+1}: {doc}")
            sources.append(meta.get("source", "unknown"))
            total_score += score

        context = "\n\n".join(context_parts)
        avg_score = total_score / len(results)

        # Step 4: ç”Ÿæˆç­”æ¡ˆ
        prompt = f"""åŸºæ–¼ä»¥ä¸‹è³‡æ–™å›ç­”å•é¡Œï¼š

{context}

å•é¡Œï¼š{question}

è«‹æ ¹æ“šæä¾›çš„è³‡æ–™æº–ç¢ºå›ç­”ã€‚å¦‚æœè³‡æ–™ä¸è¶³ï¼Œè«‹èªªæ˜ã€‚"""

        answer = self.agent(prompt)

        return {
            "answer": answer,
            "sources": list(set(sources)),
            "confidence": avg_score,
            "retrieved_docs": len(results),
        }


# === Step 5: å»ºç«‹çŸ¥è­˜åº« ===
print("=== å»ºç«‹RAGç³»çµ± ===")
print()

agent = Agent(system_prompt="ä½ æ˜¯ä¸€å€‹æº–ç¢ºçš„å•ç­”åŠ©æ‰‹ï¼Œåªæ ¹æ“šæä¾›çš„è³‡æ–™å›ç­”å•é¡Œã€‚")
embedder = Embedding()
rag = RAGSystem(agent, embedder)

# æ·»åŠ çŸ¥è­˜
knowledge_base = [
    {
        "text": """Pythonæ˜¯ä¸€ç¨®é«˜éšç¨‹å¼èªè¨€ï¼Œç”±Guido van Rossumåœ¨1991å¹´å‰µé€ ã€‚
        Pythonå¼·èª¿ç¨‹å¼ç¢¼çš„å¯è®€æ€§ï¼Œä½¿ç”¨ç¸®æ’ä¾†å®šç¾©ç¨‹å¼ç¢¼å¡Šã€‚
        Pythonæ”¯æ´å¤šç¨®ç¨‹å¼è¨­è¨ˆç¯„å¼ï¼ŒåŒ…æ‹¬ç‰©ä»¶å°å‘ã€ç¨‹åºå¼å’Œå‡½æ•¸å¼ç¨‹å¼è¨­è¨ˆã€‚
        Pythonæ“æœ‰è±å¯Œçš„æ¨™æº–åº«å’Œç¬¬ä¸‰æ–¹å¥—ä»¶ç”Ÿæ…‹ç³»çµ±ã€‚""",
        "source": "PythonåŸºç¤çŸ¥è­˜",
    },
    {
        "text": """æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹åˆ†æ”¯ï¼Œè®“é›»è…¦èƒ½å¤ å¾è³‡æ–™ä¸­å­¸ç¿’ã€‚
        ç›£ç£å¼å­¸ç¿’éœ€è¦æ¨™è¨˜çš„è¨“ç·´è³‡æ–™ï¼ŒåŒ…æ‹¬åˆ†é¡å’Œå›æ­¸ä»»å‹™ã€‚
        éç›£ç£å¼å­¸ç¿’ä¸éœ€è¦æ¨™è¨˜è³‡æ–™ï¼Œå¸¸ç”¨æ–¼èšé¡å’Œé™ç¶­ã€‚
        å¼·åŒ–å­¸ç¿’é€éèˆ‡ç’°å¢ƒäº’å‹•ä¾†å­¸ç¿’æœ€ä½³ç­–ç•¥ã€‚""",
        "source": "æ©Ÿå™¨å­¸ç¿’æ¦‚è«–",
    },
    {
        "text": """æ·±åº¦å­¸ç¿’ä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²è·¯ä¾†å­¸ç¿’è³‡æ–™çš„è¤‡é›œè¡¨ç¤ºã€‚
        å·ç©ç¥ç¶“ç¶²è·¯(CNN)æ“…é•·è™•ç†åœ–åƒè³‡æ–™ã€‚
        å¾ªç’°ç¥ç¶“ç¶²è·¯(RNN)é©åˆè™•ç†åºåˆ—è³‡æ–™å¦‚æ–‡å­—å’Œæ™‚é–“åºåˆ—ã€‚
        Transformeræ¶æ§‹é©æ–°äº†è‡ªç„¶èªè¨€è™•ç†é ˜åŸŸã€‚""",
        "source": "æ·±åº¦å­¸ç¿’æŠ€è¡“",
    },
]


for kb in knowledge_base:
    rag.add_knowledge(kb["text"], kb["source"])

print()

# === Step 6: æ¸¬è©¦å•ç­” ===
print("=== æ¸¬è©¦RAGå•ç­”ç³»çµ± ===")

questions = [
    "Pythonæ˜¯èª°å‰µé€ çš„ï¼Ÿ",
    "ä»€éº¼æ˜¯ç›£ç£å¼å­¸ç¿’ï¼Ÿ",
    "CNNé©åˆè™•ç†ä»€éº¼é¡å‹çš„è³‡æ–™ï¼Ÿ",
    "Transformerç”¨æ–¼ä»€éº¼é ˜åŸŸï¼Ÿ",
]

for q in questions:
    result = rag.answer_question(q)
    print()
    print("=" * 50)
    print(f"å•é¡Œï¼š{q}")
    print(f"ç­”æ¡ˆï¼š{result['answer']}")
    print(f"ä¾†æºï¼š{', '.join(result['sources'])}")
    print(f"ä¿¡å¿ƒåº¦ï¼š{result['confidence']:.2%}")
    print(f"æª¢ç´¢æ–‡ä»¶æ•¸ï¼š{result['retrieved_docs']}")

# === Step 7: å„²å­˜å’Œè¼‰å…¥ ===
print("\n" + "=" * 50)
print("=== å„²å­˜RAGç³»çµ± ===")

# å„²å­˜å‘é‡è³‡æ–™åº«
rag.vector_db.save("data/rag_database.json")

# === Step 8: äº’å‹•æ¨¡å¼ ===
print("\n" + "=" * 50)
print("=== äº’å‹•å•ç­”æ¨¡å¼ ===")
print("è¼¸å…¥å•é¡Œä¾†æ¸¬è©¦RAGç³»çµ±ï¼ˆè¼¸å…¥'quit'çµæŸï¼‰")
print()

while True:
    user_question = input("ğŸ‘¤ ä½ çš„å•é¡Œï¼š")

    if user_question.lower() == "quit":
        print("ğŸ‘‹ å†è¦‹ï¼")
        break

    result = rag.answer_question(user_question)
    print(f"ğŸ¤– ç­”æ¡ˆï¼š{result['answer']}")
    print(f"ğŸ“š è³‡æ–™ä¾†æºï¼š{', '.join(result['sources'])}")
    print(f"ğŸ“Š ä¿¡å¿ƒåº¦ï¼š{result['confidence']:.2%}")
    print()
