# 模型訓練方法

## Fine-tune（微調）

### 定義
Fine-tune 是在已經預訓練好的模型基礎上，使用特定領域或任務的資料進行進一步訓練，以提升模型在該任務上的表現。

### 為什麼需要 Fine-tune？
1. **領域適應**：預訓練模型可能不熟悉特定領域的術語和知識
2. **任務特化**：針對特定任務最佳化模型行為
3. **成本效益**：比從頭訓練節省大量時間和運算資源
4. **資料效率**：只需要較少的標註資料就能達到好效果

### Fine-tune 的類型

#### 1. Full Fine-tuning（全量微調）
更新模型的所有參數。

**優點**：
- 最大的靈活性
- 通常能達到最好的性能

**缺點**：
- 需要大量運算資源
- 容易過擬合（Overfitting）
- 需要較多訓練資料

#### 2. Parameter-Efficient Fine-tuning（參數高效微調）
只更新一小部分參數，其餘參數保持凍結。

**常見方法**：
- **LoRA**（Low-Rank Adaptation）：加入低秩矩陣
- **Adapter**：插入小型神經網路模組
- **Prefix Tuning**：只訓練前綴向量
- **Prompt Tuning**：只訓練提示詞的嵌入向量

**優點**：
- 節省運算資源和記憶體
- 減少過擬合風險
- 可以快速切換不同任務

#### 3. Instruction Fine-tuning（指令微調）
使用「指令-回應」格式的資料集訓練模型，使其能夠理解並遵循使用者指令。

**資料格式範例**：
```
指令：將以下句子翻譯成英文。
輸入：今天天氣真好。
輸出：The weather is really nice today.
```

### Fine-tune 流程
1. **準備資料集**：收集並標註領域特定資料
2. **選擇基礎模型**：挑選合適的預訓練模型
3. **設定超參數**：學習率、批次大小、訓練輪數等
4. **訓練**：在特定任務資料上訓練
5. **評估**：在驗證集上評估效果
6. **調整**：根據評估結果調整超參數

### 注意事項
- **學習率**：通常使用比預訓練更小的學習率
- **資料品質**：高品質資料比大量低品質資料更重要
- **過擬合**：注意監控驗證集表現，避免過度擬合訓練資料
- **災難性遺忘**：模型可能會忘記預訓練時學到的通用知識

## Instruct Model（指令模型）

### 定義
Instruct Model 是經過指令微調的模型，能夠理解並執行使用者的指令，提供符合預期的回應。

### 與一般 Fine-tune 的差異
- **目標不同**：不是針對單一任務，而是學習遵循各種指令
- **資料格式**：使用「指令-輸入-輸出」三元組
- **泛化能力**：能處理訓練時未見過的指令類型

### 訓練資料來源
1. **人工標註**：雇用標註人員撰寫高品質的指令-回應配對
2. **合成資料**：使用現有模型生成訓練資料
3. **公開資料集**：如 FLAN、SuperNaturalInstructions、Alpaca 等

### 常見的 Instruct 資料集
- **FLAN**（Finetuned Language Net）：Google 的指令微調資料集
- **Alpaca**：Stanford 使用 GPT-3.5 生成的指令資料集
- **Dolly**：Databricks 的開源指令資料集
- **OpenAssistant**：社群眾包的對話資料集

## RLHF（Reinforcement Learning from Human Feedback）

### 定義
RLHF 是一種透過人類反饋進行強化學習的訓練方法，用於讓模型的行為更符合人類偏好和價值觀。

### 為什麼需要 RLHF？
單純的指令微調可能存在以下問題：
- 模型可能生成有害、不當或偏見內容
- 回應可能不符合人類期望的風格或品質
- 難以定義「好的回應」的明確標準

### RLHF 三階段流程

#### 階段一：Supervised Fine-tuning (SFT)
使用高品質的人工標註資料進行指令微調，得到初始的 Instruct Model。

**目標**：讓模型學會基本的指令遵循能力。

#### 階段二：訓練 Reward Model（獎勵模型）
1. 對同一個提示詞，讓 SFT 模型生成多個不同的回應
2. 人類標註者對這些回應進行排序（哪個更好）
3. 訓練一個 Reward Model 來預測人類偏好

**目標**：建立一個能夠評估回應品質的自動化評分系統。

#### 階段三：強化學習最佳化
使用 PPO（Proximal Policy Optimization）等強化學習演算法：
1. 模型生成回應
2. Reward Model 給予評分
3. 根據評分調整模型參數
4. 重複以上過程

**目標**：讓模型生成獲得高分（人類偏好）的回應。

### RLHF 的關鍵技術

#### 1. Reward Model
一個分類或回歸模型，輸入「提示詞 + 回應」，輸出品質分數。

#### 2. PPO（Proximal Policy Optimization）
一種穩定的強化學習演算法，避免模型更新幅度過大。

#### 3. KL Divergence Penalty
防止模型偏離太遠，保持與原始 SFT 模型的相似性：
```
Loss = -Reward + β * KL(π_RL || π_SFT)
```

### RLHF 的挑戰
1. **人類反饋成本高**：需要大量人工標註
2. **獎勵 Hacking**：模型可能找到意外的方式獲得高分
3. **偏好多樣性**：不同人有不同偏好，難以統一
4. **訓練不穩定**：強化學習訓練較難調整

### 替代方案
- **RLAIF**（RL from AI Feedback）：用 AI 取代人類提供反饋
- **DPO**（Direct Preference Optimization）：直接最佳化偏好，不需要訓練 Reward Model
- **Constitutional AI**：透過一組原則指導模型行為

## 訓練方法比較

| 方法 | 資料需求 | 運算成本 | 效果 | 難度 |
|------|----------|----------|------|------|
| Fine-tune | 中等 | 中等 | 針對特定任務優秀 | 中等 |
| Instruct Fine-tuning | 較高 | 中等 | 廣泛的指令遵循 | 中等 |
| RLHF | 非常高 | 非常高 | 符合人類偏好 | 高 |

## 實務建議

### 選擇訓練方法
1. **有明確任務**：使用 Fine-tune
2. **需要通用指令遵循**：使用 Instruct Fine-tuning
3. **需要高度對齊人類偏好**：使用 RLHF（通常是大公司的選擇）

### 對於一般開發者
- 使用已經過 Instruct 或 RLHF 訓練的模型（如 ChatGPT、Llama-2-Chat）
- 透過 Prompt Engineering 調整模型行為
- 必要時進行輕量級的 Fine-tune（如 LoRA）

### 資源有限時
- 優先考慮 Few-shot Learning 或 In-context Learning
- 使用較小的模型進行 Fine-tune（如 LLaMA 7B、Gemma 7B）
- 採用參數高效微調方法（LoRA、Adapter）

